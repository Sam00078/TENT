{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TT_experiments_weights.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "7PaY6KZmbfBP"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yy3gc13nHMR"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "! pip install -q pyyaml h5py  # Required to save models in HDF5 format"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6NYTcAvln4J"
      },
      "source": [
        "### Mount Google Drive\n",
        "\n",
        "**Requires dataset_tensor.npy file in \"Colab Notebooks/Tensorized Transformers/Data\" folder!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaNZqkhqfjiJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b36468a-d23b-438f-e61f-5fdc28fcffef"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "PATH = '/content/drive/My Drive/Colab Notebooks/Tensorized Transformers/'\n",
        "DATA_PATH = PATH + 'Data/'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHxUZWqils8I"
      },
      "source": [
        "### Clone Tensorized Transformers github repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmKol2g8WU1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21dec394-1155-4fbe-b2c9-d35171b79098"
      },
      "source": [
        "print('Github username:')\n",
        "git_username = %sx read -p ''\n",
        "git_username = git_username[0]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Github username:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkU2f9HhZQB9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9469db89-5bfa-46ce-d0e1-aeb58f5e4e57"
      },
      "source": [
        "print('Github access token (https://github.com/settings/tokens):')\n",
        "git_token = %sx read -p ''\n",
        "git_token = git_token[0]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Github access token (https://github.com/settings/tokens):\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KA6Ej0xOjzbu"
      },
      "source": [
        "# Clone the entire repo.\n",
        "%cd /content\n",
        "!git clone -l -s https://$git_username:$git_token@github.com/onurbil/tensorized_transformers.git tensorized_transformers\n",
        "%cd tensorized_transformers\n",
        "!ls\n",
        "%cd ..\n",
        "\n",
        "REPO_PATH = '/content/tensorized_transformers'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmfy2j6FkjLo"
      },
      "source": [
        "import sys\n",
        "sys.path.append(REPO_PATH)\n",
        "print(sys.path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl9e5tQalzjp"
      },
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUxsxY5wlBzO"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as kr\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "\n",
        "import model.tt_mod_weights as tt ###### MODEL 1 SHOULD CHANGE TO ---> import model.tensorized_transformer as tt\n",
        "import dataset_tools.split\n",
        "from visualization_tools.visualization import visualize_pos_encoding, attention_plotter\n",
        "\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "import datetime\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir '/notebooks/output'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4agD0rDHEfoo"
      },
      "source": [
        "dir = '/notebooks/output/'  + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\n",
        "# Load dataset:\r\n",
        "filename = DATA_PATH + 'dataset_tensor.npy'\r\n",
        "# file_path = os.path.join(common.paths.PROCESSED_DATASET_DIR, filename)\r\n",
        "dataset = np.load(filename, allow_pickle=True)\r\n",
        "\r\n",
        "print(dataset.shape)\r\n",
        "\r\n",
        "###### ALL PARAMETERS HERE######:\r\n",
        "softmax_type = 2\r\n",
        "input_length = 16\r\n",
        "lag = 4\r\n",
        "epoch = 100\r\n",
        "\r\n",
        "learning_rate = 0.001\r\n",
        "head_num = 16\r\n",
        "d_model = 32\r\n",
        "dense_units = 64\r\n",
        "batch_size = 64\r\n",
        "\r\n",
        "num_examples = 10000\r\n",
        "num_valid_examples = 500\r\n",
        "initializer = 'RandomNormal'\r\n",
        "\r\n",
        "train, test = dataset_tools.split.split_train_test(dataset)\r\n",
        "x_train, y_train = dataset_tools.split.get_xy(train, input_length=input_length, lag=lag)\r\n",
        "x_test, y_test = dataset_tools.split.get_xy(test, input_length=input_length, lag=lag)\r\n",
        "\r\n",
        "#x_train = x_train.astype('float32')\r\n",
        "x_train = tf.reshape(x_train, (x_train.shape[0], x_train.shape[1], dataset.shape[1], dataset.shape[2]))\r\n",
        "y_train = tf.reshape(y_train, (y_train.shape[0], dataset.shape[1], dataset.shape[2]))\r\n",
        "x_test = tf.reshape(x_test, (x_test.shape[0], x_test.shape[1], dataset.shape[1], dataset.shape[2]))\r\n",
        "y_test = tf.reshape(y_test, (y_test.shape[0], dataset.shape[1], dataset.shape[2]))\r\n",
        "\r\n",
        "# Choosing first 29 cities\r\n",
        "x_train = x_train[:, :, :29, :]\r\n",
        "y_train = y_train[:, :29, :]\r\n",
        "x_test = x_test[:, :, :29, :]\r\n",
        "y_test = y_test[:, :29, :]\r\n",
        "\r\n",
        "input_shape = (input_length, x_train.shape[-2], x_train.shape[-1])\r\n",
        "output_shape = (1, 1)\r\n",
        "\r\n",
        "# Choosing temperature as output\r\n",
        "y_train = y_train[..., 0, 4]\r\n",
        "y_test = y_test[..., 0, 4]\r\n",
        "\r\n",
        "print(f'x_train.shape: {x_train.shape}')\r\n",
        "print(f'x_test.shape: {x_test.shape}')\r\n",
        "\r\n",
        "model = kr.Sequential([\r\n",
        "    kr.Input(shape=input_shape),\r\n",
        "    tt.PositionalEncoding(broadcast=True),\r\n",
        "    tt.EncoderLayer(input_length, d_model, head_num, dense_units, initializer, softmax_type),\r\n",
        "    tt.EncoderLayer(input_length, d_model, head_num, dense_units, initializer, softmax_type),\r\n",
        "    tt.EncoderLayer(input_length, d_model, head_num, dense_units, initializer, softmax_type),\r\n",
        "    tt.EncoderLayer(input_length, d_model, head_num, dense_units, initializer, softmax_type),\r\n",
        "    tt.EncoderLayer(input_length, d_model, head_num, dense_units, initializer, softmax_type),\r\n",
        "    tt.EncoderLayer(input_length, d_model, head_num, dense_units, initializer, softmax_type),\r\n",
        "    kr.layers.Flatten(),\r\n",
        "    kr.layers.Dense(tf.reduce_prod(output_shape), activation='linear'),\r\n",
        "    kr.layers.Reshape(output_shape),\r\n",
        "])\r\n",
        "\r\n",
        "model.summary()\r\n",
        "model.compile(optimizer=kr.optimizers.Adam(learning_rate=learning_rate), loss='mse', metrics=['mae'])\r\n",
        "\r\n",
        "x_valid = x_train[-num_examples - num_valid_examples:-num_examples, ...]\r\n",
        "y_valid = y_train[-num_examples - num_valid_examples:-num_examples]\r\n",
        "print(f'x_valid.shape: {x_valid.shape}')\r\n",
        "\r\n",
        "x_train = x_train[-num_examples:]\r\n",
        "y_train = y_train[-num_examples:]\r\n",
        "\r\n",
        "# Callbacks\r\n",
        "print_attention_weights = kr.callbacks.LambdaCallback(\r\n",
        "    on_train_end=lambda batch: print(model.layers[1].attention_weights))\r\n",
        "early_stopping = kr.callbacks.EarlyStopping(patience=10,\r\n",
        "                                            restore_best_weights=True,\r\n",
        "                                            verbose=1)\r\n",
        "\r\n",
        "model.fit(\r\n",
        "    x_train, y_train,\r\n",
        "    epochs=epoch,\r\n",
        "    batch_size=batch_size,\r\n",
        "    validation_data=(x_valid, y_valid),\r\n",
        "    callbacks=[early_stopping, TensorBoard(log_dir=dir)]\r\n",
        ")\r\n",
        "\r\n",
        "pred = model.predict(x_test[0:10])\r\n",
        "labels = np.arange(model.layers[1].attention_weights.shape[-2]).tolist()\r\n",
        "attention_plotter(tf.reshape(model.layers[1].attention_weights[1][0], (input_length, -1)), labels)\r\n",
        "attention_plotter(tf.reshape(model.layers[1].attention_weights[2][0], (input_length, -1)), labels)\r\n",
        "attention_plotter(tf.reshape(model.layers[1].attention_weights[3][0], (input_length, -1)), labels)\r\n",
        "\r\n",
        "preds = []\r\n",
        "for i in range(x_valid.shape[0]):\r\n",
        "    if (i + 1) % 100 == 0:\r\n",
        "        print(f'prediction: {i + 1}/{x_valid.shape[0]}')\r\n",
        "    preds.append(model.predict(x_valid[i][np.newaxis, ...]))\r\n",
        "pred = np.concatenate(preds, axis=0)\r\n",
        "mse = np.mean(kr.metrics.mse(y_valid, pred))\r\n",
        "mae = np.mean(kr.metrics.mae(y_valid, pred))\r\n",
        "print(f'mse: {mse}, mae: {mae}')\r\n",
        "\r\n",
        "plt.figure(figsize=(14, 8))\r\n",
        "plt.plot(range(pred.size), pred.flatten(), label='pred')\r\n",
        "plt.plot(range(len(y_valid)), y_valid, label='true')\r\n",
        "plt.legend()\r\n",
        "plt.show()\r\n",
        "\r\n",
        "print(\"\\n\\n######################## Model description ################################\")\r\n",
        "model.summary()\r\n",
        "print(\"softmax_type = \", softmax_type)\r\n",
        "print(\"Input_length = \", input_length)\r\n",
        "print(\"Lag = \", lag)\r\n",
        "print(\"Epoch = \", epoch)\r\n",
        "\r\n",
        "print(\"LR = \", learning_rate)\r\n",
        "print(\"Head_num = \", head_num)\r\n",
        "print(\"d_model = \", d_model)\r\n",
        "print(\"dense_units = \", dense_units)\r\n",
        "print(\"batch_size = \", batch_size)\r\n",
        "\r\n",
        "print(\"num_examples = \", num_examples)\r\n",
        "print(\"num_valid_examples = \", num_valid_examples)\r\n",
        "print(\"input_shape = \", input_shape)\r\n",
        "\r\n",
        "pred = model.predict(x_test)\r\n",
        "mae = kr.metrics.mae(y_test.numpy().flatten(), pred.flatten())\r\n",
        "print(\"\\n\\n######################## Results ##########################################\")\r\n",
        "print(f'test mae: {np.mean(mae)}')\r\n",
        "\r\n",
        "### Saving Model:\r\n",
        "model.save('/content/drive/My Drive/Colab Notebooks/Model/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\r\n",
        "# TO get it back\r\n",
        "# new_model = tf.keras.models.load_model('saved_model/my_model')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}