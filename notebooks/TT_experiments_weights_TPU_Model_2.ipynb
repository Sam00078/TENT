{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TT_experiments_weights_TPU_Model_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yy3gc13nHMR"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        " \n",
        "! pip install -q pyyaml h5py  # Required to save models in HDF5 format"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6NYTcAvln4J"
      },
      "source": [
        "### Mount Google Drive\n",
        "\n",
        "**Requires dataset_tensor.npy file in \"Colab Notebooks/Tensorized Transformers/Data\" folder!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaNZqkhqfjiJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1e3970f-07c3-4d2b-e17d-c681e775985c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "PATH = '/content/drive/My Drive/Colab Notebooks/Tensorized Transformers/'\n",
        "DATA_PATH = PATH + 'Data/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHxUZWqils8I"
      },
      "source": [
        "### Clone Tensorized Transformers github repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmKol2g8WU1d"
      },
      "source": [
        "print('Github username:')\n",
        "git_username = %sx read -p ''\n",
        "git_username = git_username[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkU2f9HhZQB9"
      },
      "source": [
        "print('Github access token (https://github.com/settings/tokens):')\n",
        "git_token =  %sx read -p ''\n",
        "git_token = git_token[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KA6Ej0xOjzbu"
      },
      "source": [
        "# Clone the entire repo.\n",
        "%cd /content\n",
        "!git clone -l -s https://$git_username:$git_token@github.com/onurbil/tensorized_transformers.git tensorized_transformers\n",
        "%cd tensorized_transformers\n",
        "!ls\n",
        "%cd ..\n",
        "\n",
        "REPO_PATH = '/content/tensorized_transformers'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmfy2j6FkjLo"
      },
      "source": [
        "import sys\n",
        "sys.path.append(REPO_PATH)\n",
        "print(sys.path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl9e5tQalzjp"
      },
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUxsxY5wlBzO"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as kr\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import model.tt_mode_weights_TPU as tt\n",
        "import dataset_tools.split\n",
        "from visualization_tools.visualization import visualize_pos_encoding, attention_plotter\n",
        "\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "import datetime\n",
        "\n",
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir '/content/drive/My Drive/Colab Notebooks/Tensorized Transformers/output/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O8WyQhzZO2_"
      },
      "source": [
        "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\r\n",
        "print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\r\n",
        "\r\n",
        "tf.config.experimental_connect_to_cluster(tpu)\r\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\r\n",
        "\r\n",
        "strategy = tf.distribute.TPUStrategy(tpu)#tf.distribute.experimental.TPUStrategy(tpu)\r\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFJ-r1LJaj_a"
      },
      "source": [
        "dir = '/content/drive/My Drive/Colab Notebooks/Tensorized Transformers/output/'  + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# Load dataset:\n",
        "filename = DATA_PATH + 'dataset_tensor.npy'\n",
        "# file_path = os.path.join(common.paths.PROCESSED_DATASET_DIR, filename)\n",
        "dataset = np.load(filename, allow_pickle=True)\n",
        "\n",
        "print(dataset.shape)\n",
        "\n",
        "###### ALL PARAMETERS HERE######:\n",
        "softmax_type = 2\n",
        "input_length = 16\n",
        "lag = 4\n",
        "epoch = 200\n",
        "\n",
        "d_model = 256\n",
        "warmup_steps = 50\n",
        "factor1=-0.6\n",
        "factor2=-1.5\n",
        "\n",
        "learning_rate = 0.0001\n",
        "head_num = 64\n",
        "d_model = 256\n",
        "dense_units = 512\n",
        "batch_size = 4\n",
        "\n",
        "num_examples = 10112 \n",
        "num_valid_examples = 512\n",
        "initializer = 'RandomNormal'\n",
        "patience = 60\n",
        "\n",
        "num_examples = (num_examples // batch_size) * batch_size\n",
        "num_valid_examples = (num_valid_examples // batch_size) * batch_size\n",
        "\n",
        "train, test = dataset_tools.split.split_train_test(dataset)\n",
        "x_train, y_train = dataset_tools.split.get_xy(train, input_length=input_length, lag=lag)\n",
        "x_test, y_test = dataset_tools.split.get_xy(test, input_length=input_length, lag=lag)\n",
        "\n",
        "#x_train = x_train.astype('float32')\n",
        "x_train = tf.reshape(x_train, (x_train.shape[0], x_train.shape[1], dataset.shape[1], dataset.shape[2]))\n",
        "y_train = tf.reshape(y_train, (y_train.shape[0], dataset.shape[1], dataset.shape[2]))\n",
        "x_test = tf.reshape(x_test, (x_test.shape[0], x_test.shape[1], dataset.shape[1], dataset.shape[2]))\n",
        "y_test = tf.reshape(y_test, (y_test.shape[0], dataset.shape[1], dataset.shape[2]))\n",
        "\n",
        "# Choosing first 29 cities\n",
        "x_train = x_train[:, :, :29, :]\n",
        "y_train = y_train[:, :29, :]\n",
        "x_test = x_test[:, :, :29, :]\n",
        "y_test = y_test[:, :29, :]\n",
        "\n",
        "print(f'FULL_x_train.shape: {x_train.shape}')\n",
        "\n",
        "input_shape = (input_length, x_train.shape[-2], x_train.shape[-1])\n",
        "output_shape = (1, 1)\n",
        "\n",
        "# Choosing temperature as output\n",
        "y_train = y_train[..., 0, 4]\n",
        "y_test = y_test[..., 0, 4]\n",
        "\n",
        "learning_rate = tt.CustomSchedule(d_model, warmup_steps=warmup_steps, factor1=factor1, factor2=factor2) #tt.CustomSchedule(d_model)                 # , warmup_steps=50, factor1=-0.84, factor2=-1.7)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, \n",
        "                                     beta_1=0.9, \n",
        "                                     beta_2=0.98, \n",
        "                                     epsilon=1e-9\n",
        "                                     )\n",
        "lr_metric = tt.get_lr_metric(optimizer)\n",
        "\n",
        "# optimizer = tf.keras.optimizers.Adadelta(learning_rate)\n",
        "# optimizer = tf.keras.optimizers.Nadam(learning_rate)\n",
        "\n",
        "temp_learning_rate_schedule = tt.CustomSchedule(d_model, warmup_steps=warmup_steps, factor1=factor1, factor2=factor2)\n",
        "plt.plot(temp_learning_rate_schedule(tf.range(20000, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "##tf.keras.optimizers.Adadelta\n",
        "\n",
        "with strategy.scope():\n",
        "  model = kr.Sequential([\n",
        "              kr.Input(shape=input_shape),\n",
        "              tt.PositionalEncoding(broadcast=True),\n",
        "              tt.EncoderLayer(input_length, d_model, head_num, dense_units, initializer, softmax_type, batch_size),\n",
        "              tt.EncoderLayer(input_length, d_model, head_num, dense_units, initializer, softmax_type, batch_size),\n",
        "              tt.EncoderLayer(input_length, d_model, head_num, dense_units, initializer, softmax_type, batch_size),\n",
        "              # tt.EncoderLayer(input_length, d_model, head_num, dense_units, initializer, softmax_type, batch_size),\n",
        "              # tt.EncoderLayer(input_length, d_model, head_num, dense_units, initializer, softmax_type, batch_size),\n",
        "              # tt.EncoderLayer(input_length, d_model, head_num, dense_units, initializer, softmax_type, batch_size),\n",
        "              kr.layers.Flatten(),\n",
        "              kr.layers.Dense(tf.reduce_prod(output_shape), activation='linear'),\n",
        "              kr.layers.Reshape(output_shape),\n",
        "              ])\n",
        "  model.compile(optimizer=optimizer, loss='mae', metrics=['mse', lr_metric])\n",
        "  #model.compile(optimizer=kr.optimizers.Adam(learning_rate=learning_rate), loss='mse', metrics=['mae'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "x_valid = x_train[-num_examples - num_valid_examples:-num_examples, ...]\n",
        "y_valid = y_train[-num_examples - num_valid_examples:-num_examples]\n",
        "print(f'x_valid.shape: {x_valid.shape}')\n",
        "\n",
        "x_train = x_train[-num_examples:]\n",
        "y_train = y_train[-num_examples:]\n",
        "\n",
        "print(f'x_train.shape: {x_train.shape}')\n",
        "print(f'x_test.shape: {x_test.shape}')\n",
        "\n",
        "# Callbacks\n",
        "print_attention_weights = kr.callbacks.LambdaCallback(\n",
        "    on_train_end=lambda batch: print(model.layers[1].attention_weights))\n",
        "early_stopping = kr.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                            patience=patience,\n",
        "                                            restore_best_weights=True,\n",
        "                                            verbose = 1)\n",
        "\n",
        "model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=epoch,\n",
        "    batch_size=batch_size * 8,\n",
        "    validation_data=(x_valid, y_valid),\n",
        "    callbacks=[early_stopping]\n",
        "         )\n",
        "#TensorBoard(log_dir=dir), \n",
        "# labels = np.arange(model.layers[1].attention_weights.shape[-2]).tolist()\n",
        "\n",
        "# if (softmax_type == 1 or softmax_type == 2):\n",
        "#     attention_plotter(tf.reshape(model.layers[1].attention_weights[1][0], (input_length,-1)), labels)\n",
        "#     attention_plotter(tf.reshape(model.layers[1].attention_weights[2][0], (input_length,-1)), labels)\n",
        "#     attention_plotter(tf.reshape(model.layers[1].attention_weights[3][0], (input_length,-1)), labels)        \n",
        "#     attention_plotter(tf.reshape(model.layers[1].attention_weights[4][0], (input_length,-1)), labels)        \n",
        "\n",
        "# elif softmax_type == 3:\n",
        "#     # print(model.layers[1].attention_weights[0][3].numpy())\n",
        "#     attention_3d_plotter(model.layers[1].attention_weights[0][3].numpy(), city_labels)\n",
        "# else:\n",
        "#     pass\n",
        "\n",
        "pred = model.predict(x_valid)\n",
        "mae = kr.metrics.mae(y_valid.numpy().flatten(), pred.flatten())\n",
        "print(f'Figure mae: {np.mean(mae)}')\n",
        "\n",
        "plt.figure(figsize=(20, 8))\n",
        "plt.plot(range(pred.size), pred.flatten(), label='pred')\n",
        "plt.plot(range(len(y_valid)), y_valid, label='true')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\\n######################## Model description ################################\")\n",
        "model.summary()\n",
        "print(\"softmax_type = \", softmax_type)\n",
        "print(\"Input_length = \", input_length)\n",
        "print(\"Lag = \", lag)\n",
        "print(\"Epoch = \", epoch)\n",
        "print(\"warmup_steps = \", warmup_steps)\n",
        "print(\"factor1 = \", factor1)\n",
        "print(\"factor2 = \", factor2)\n",
        "\n",
        "print(\"LR = \", learning_rate)\n",
        "print(\"Head_num = \", head_num)\n",
        "print(\"d_model = \", d_model)\n",
        "print(\"dense_units = \", dense_units)\n",
        "print(\"batch_size = \", batch_size)\n",
        "\n",
        "print(\"num_examples = \", num_examples)\n",
        "print(\"num_valid_examples = \", num_valid_examples)\n",
        "print(\"input_shape = \", input_shape)\n",
        "print(\"patience = \", patience)\n",
        "\n",
        "pred = model.predict(x_test[-(8813//batch_size)*batch_size:, ...])\n",
        "mae = kr.metrics.mae(y_test[-(8813//batch_size)*batch_size:, ...].numpy().flatten(), pred.flatten())\n",
        "print(\"\\n\\n######################## Results ##########################################\")\n",
        "print(f'test mae: {np.mean(mae)}')\n",
        "\n",
        "### Saving Model:\n",
        "# model.save('/content/drive/My Drive/Colab Notebooks/Model/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "# TO get it back\n",
        "# new_model = tf.keras.models.load_model('saved_model/my_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFIypn_H1Bzu"
      },
      "source": [
        "## **Test of the learning rate curve**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IAKcWNmkTNC"
      },
      "source": [
        "d_model = 256\r\n",
        "warmup_steps = 50\r\n",
        "factor1=-0.6\r\n",
        "factor2=-1.5\r\n",
        "\r\n",
        "temp_learning_rate_schedule = tt.CustomSchedule(d_model, warmup_steps=warmup_steps, factor1=factor1, factor2=factor2)\r\n",
        "plt.figure(figsize=(20, 8))\r\n",
        "plt.plot(temp_learning_rate_schedule(tf.range(10000, dtype=tf.float32)))\r\n",
        "plt.ylabel(\"Learning Rate\")\r\n",
        "plt.xlabel(\"Train Step\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PaY6KZmbfBP"
      },
      "source": [
        "#### Old experiments"
      ]
    }
  ]
}