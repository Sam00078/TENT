{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "TT_All_models_experiments.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "f7O53vu1PFna",
    "FR-UFRAxEGPs"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6NYTcAvln4J"
   },
   "source": [
    "### Mount Google Drive\n",
    "\n",
    "**Requires dataset_tensor.npy file in \"Colab Notebooks/Tensorized Transformers/Data\" folder!**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1XNfw-0EfZ4B"
   },
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import scipy.io\r\n",
    "import sklearn.model_selection\r\n",
    "import datetime\r\n",
    "\r\n",
    "! pip install -q pyyaml h5py  # Required to save models in HDF5 format\r\n",
    "! pip install torch\r\n",
    "! pip install einops\r\n",
    "! pip install tqdm\r\n",
    "! pip install torchsummary\r\n",
    "! pip install scipy"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oaNZqkhqfjiJ"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "PATH = '/content/drive/My Drive/Colab Notebooks/Tensorized Transformers/'\n",
    "DATA_PATH = PATH + 'Data/'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHxUZWqils8I"
   },
   "source": [
    "### Clone Tensorized Transformers and multidim conv github repository"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BmKol2g8WU1d"
   },
   "source": [
    "print('Github username:')\n",
    "git_username = %sx read -p ''\n",
    "git_username = git_username[0]\n",
    "\n",
    "print('Github access token (https://github.com/settings/tokens):')\n",
    "git_token = %sx read -p ''\n",
    "git_token = git_token[0]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KA6Ej0xOjzbu"
   },
   "source": [
    "# Clone the entire repo.\n",
    "%cd /content\n",
    "!git clone -l -s https://$git_username:$git_token@github.com/onurbil/tensorized_transformers.git tensorized_transformers\n",
    "%cd tensorized_transformers\n",
    "!ls\n",
    "%cd ..\n",
    "\n",
    "%cd /content\n",
    "!git clone -l -s https://github.com/onurbil/multidim_conv.git sc\n",
    "%cd sc\n",
    "!ls\n",
    "%cd .."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jmfy2j6FkjLo"
   },
   "source": [
    "import sys\n",
    "\n",
    "TT_REPO_PATH = '/content/tensorized_transformers'\n",
    "SC_REPO_PATH = '/content/sc'\n",
    "\n",
    "sys.path.append(TT_REPO_PATH)\n",
    "sys.path.append(SC_REPO_PATH)\n",
    "print(sys.path)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WF87tWVHk3l6"
   },
   "source": [
    "### Get Kaggle data and save it to your Drive\r\n",
    "\r\n",
    "** Only if you don't have it saved in your drive or want to update it **"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NuLsnW-uRIyf"
   },
   "source": [
    "import shutil\r\n",
    "import os\r\n",
    "from common.paths import PROCESSED_DATASET_DIR\r\n",
    "from tensorized_transformers import main\r\n",
    "\r\n",
    "filesToMove = ['dataset_tensor.npy',\r\n",
    "               'scale.npy']\r\n",
    "os.makedirs(os.path.dirname(DATA_PATH), exist_ok=True)\r\n",
    "for files in filesToMove:\r\n",
    "  shutil.copy(PROCESSED_DATASET_DIR + '/' + files, DATA_PATH)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpUIw2p5-ZG9"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WkBupG-LnbX1"
   },
   "source": [
    "import dataset_tools.split\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "def get_usa_dataset(lag, step, y_feature, y_city, start_city=0, end_city=30, remove_last_from_test=0, valid_split=None, split_random=None):\n",
    "  filename = DATA_PATH + 'dataset_tensor.npy'\n",
    "  dataset = np.load(filename, allow_pickle=True)\n",
    "\n",
    "  print(dataset.shape)\n",
    "\n",
    "  train, test = dataset_tools.split.split_train_test(dataset)\n",
    "  x_train, y_train = dataset_tools.split.get_xy(train, input_length=lag, pred_time=step)\n",
    "  x_test, y_test = dataset_tools.split.get_xy(test, input_length=lag, pred_time=step)\n",
    "\n",
    "  print(x_train.shape, y_train.shape)\n",
    "  if valid_split is not None:\n",
    "    x_train, x_valid, y_train, y_valid = sklearn.model_selection.train_test_split(x_train, y_train, test_size=valid_split, random_state=split_random)\n",
    "    x_valid = np.reshape(x_valid, (x_valid.shape[0], x_valid.shape[1], dataset.shape[1], dataset.shape[2]))\n",
    "    y_valid = np.reshape(y_valid, (y_valid.shape[0], dataset.shape[1], dataset.shape[2]))\n",
    "    x_valid = x_valid[:, :, start_city:end_city, :]\n",
    "    y_valid = y_valid[:, start_city:end_city, :]\n",
    "    y_valid = np.expand_dims(y_valid[..., y_city, y_feature], axis=-1)\n",
    "\n",
    "  x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], dataset.shape[1], dataset.shape[2]))\n",
    "  y_train = np.reshape(y_train, (y_train.shape[0], dataset.shape[1], dataset.shape[2]))\n",
    "  x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], dataset.shape[1], dataset.shape[2]))\n",
    "  y_test = np.reshape(y_test, (y_test.shape[0], dataset.shape[1], dataset.shape[2]))\n",
    "\n",
    "  x_train = x_train[:, :, start_city:end_city, :]\n",
    "  y_train = y_train[:, start_city:end_city, :]\n",
    "  x_test = x_test[:-remove_last_from_test, :, start_city:end_city, :]\n",
    "  y_test = y_test[:-remove_last_from_test, start_city:end_city, :]\n",
    "\n",
    "  print(f'FULL_x_train.shape: {x_train.shape}')\n",
    "\n",
    "  y_train = np.expand_dims(y_train[..., y_city, y_feature], axis=-1)\n",
    "  y_test = np.expand_dims(y_test[..., y_city, y_feature], axis=-1)\n",
    "\n",
    "  if valid_split is None:\n",
    "    return x_train, y_train, x_test, y_test\n",
    "  else:\n",
    "    return x_train, y_train, x_valid, y_valid, x_test, y_test\n",
    "\n",
    "def get_dataset(step, feature, y_city=None, valid_split=None, split_random=None):\n",
    "  filename = DATA_PATH + f'Denmark/{feature}/step{step}.mat'\n",
    "  mat = scipy.io.loadmat(filename)\n",
    "  Xtr = mat['Xtr'].swapaxes(1, 2)\n",
    "  Ytr = mat['Ytr']\n",
    "  Xtest = mat['Xtest'].swapaxes(1, 2)\n",
    "  Ytest = mat['Ytest']\n",
    "  if y_city is not None:\n",
    "    Ytr = Ytr[:, y_city:y_city + 1]\n",
    "    Ytest = Ytest[:, y_city:y_city + 1]\n",
    "\n",
    "  if valid_split is None:\n",
    "    return Xtr, Ytr, Xtest, Ytest\n",
    "  else:\n",
    "    Xtr, Xvalid, Ytr, Yvalid = sklearn.model_selection.train_test_split(Xtr, Ytr, test_size=valid_split, random_state=split_random)\n",
    "    return Xtr, Ytr, Xvalid, Yvalid, Xtest, Ytest\n",
    "\n",
    "\n",
    "def to_flatten_dataset(Xtr, Xtest, Xvalid=None):\n",
    "  t = Xtr.shape[1]\n",
    "  f = Xtr.shape[2] * Xtr.shape[3]\n",
    "\n",
    "  Xtr = Xtr.reshape((Xtr.shape[0], t, f))\n",
    "  Xtest = Xtest.reshape((Xtest.shape[0], t, f))\n",
    "  if Xvalid is None:\n",
    "    return Xtr, Xtest\n",
    "  else:\n",
    "    print(Xvalid.shape)\n",
    "    Xvalid = Xvalid.reshape((Xvalid.shape[0], t, f))\n",
    "    return Xtr, Xtest, Xvalid\n",
    "\n",
    "\n",
    "def reshape_to_batches(Xs, Ys, batch_size):\n",
    "  num_batches = Xs.shape[0] // batch_size\n",
    "  batched_len = num_batches * batch_size\n",
    "  print(Ys.shape)\n",
    "\n",
    "  Xs = Xs[:batched_len, ...].reshape((num_batches, batch_size) + Xs.shape[1:])\n",
    "  Ys = Ys[:batched_len, ...].reshape((num_batches, batch_size) + Ys.shape[1:])\n",
    "\n",
    "  return Xs, Ys\n",
    "\n",
    "def plot_predictions(Ys, pred, folder, name, ending):\n",
    "  fileName = folder + name \n",
    "  pred = pred.flatten()\n",
    "  Ys = Ys.flatten()\n",
    "  mae = kr.metrics.mae(Ys, pred)\n",
    "  mse = kr.metrics.mse(Ys, pred)\n",
    "  f = open(fileName + \".txt\", \"a\")\n",
    "  print(f'Figure mae{ending}: {np.mean(mae)}', file=f)\n",
    "  print(f'Figure mse{ending}: {np.mean(mse)}', file=f)\n",
    "  print('\\n\\n', file=f)\n",
    "  f.close()\n",
    "  print(f'Figure mae: {np.mean(mae)}')\n",
    "  print(f'Figure mse: {np.mean(mse)}')\n",
    "\n",
    "  plot_width = 20 if Ys.size < 1000 else 100\n",
    "  plt.figure(figsize=(plot_width, 8))\n",
    "  plt.plot(range(pred.size), pred, label='pred')\n",
    "  plt.plot(range(len(Ys)), Ys, label='true')\n",
    "  plt.legend()\n",
    "  plt.savefig(fileName + ending)\n",
    "  plt.show()\n",
    "  \n",
    "def save_results(listPrintables, model, folder, name):\n",
    "  fileName = folder + name \n",
    "\n",
    "  os.makedirs(os.path.dirname(folder), exist_ok=True)\n",
    "  with open(fileName + \".txt\", \"a\") as fh:\n",
    "    model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "  f = open(fileName + \".txt\", \"a\")\n",
    "  print(\"\\n\\n######################## Model description ################################\", file=f)\n",
    "  for name, results in listPrintables:\n",
    "    print(str(name) + \" = \" + str(results), file=f)\n",
    "  print(\"######################### Results ##########################################\", file=f)\n",
    "  f.close()\n",
    "\n",
    "\n",
    "# Xtr, Ytr, Xvalid, Yvalid, Xtest, Ytest = get_usa_dataset(lag=16, step=4, y_feature=3, y_city=0, start_city=0, end_city=30, remove_last_from_test=800, valid_split=1024, split_random=None)\n",
    "# Xtr, Xtest, Xvalid = to_flatten_dataset(Xtr, Xtest, Xvalid)\n",
    "# Xtr, Ytr = reshape_to_batches(Xtr, Ytr, 128)\n",
    "# print(Xtr.shape, Ytr.shape, Xtest.shape, Ytest.shape, Xvalid.shape, Yvalid.shape)\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(100, 8))\n",
    "# plt.plot(range(Ytest.size), Ytest.flatten())\n",
    "# plt.show()"
   ],
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_rBLP7IqqSc"
   },
   "source": [
    "## DATASET\r\n",
    "\r\n",
    "** Choose your dataset **"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WQpa_CcjqySD"
   },
   "source": [
    "dataset = \"Kaggle\"  # Kaggle, Denmark\r\n",
    "\r\n",
    "\r\n",
    "if dataset = \"Denmark\":\r\n",
    "  # USA+Canada\r\n",
    "  lag = 16\r\n",
    "  step = 4 # prediction time [h]\r\n",
    "  y_feature = 4 # 4=temperature, 7=wind\r\n",
    "  y_city = 0 # 0=Vancouver\r\n",
    "  remove_last_from_test = 800\r\n",
    "  end_city = 29 # or better 30, number of cities in X\r\n",
    "  valid_split = 1024\r\n",
    "\r\n",
    "  prediction_time = step\r\n",
    "\r\n",
    "  Xtr, Ytr, Xvalid, Yvalid, Xtest, Ytest = get_usa_dataset(lag=lag, step=step, \r\n",
    "                                                          y_feature=y_feature, y_city=y_city, \r\n",
    "                                                          start_city=0, end_city=end_city, \r\n",
    "                                                          remove_last_from_test=remove_last_from_test, \r\n",
    "                                                          valid_split=valid_split, split_random=1337)\r\n",
    "\r\n",
    "if dataset = \"Kaggle\":\r\n",
    "  feature = 'wind'  # 'temp' or 'wind'\r\n",
    "  step = 1  # 1=6h, 2=12h, 3=18h, 4=24h\r\n",
    "  city = 0  # None=all or 0, 1, 2\r\n",
    "\r\n",
    "  prediction_time = step * 6\r\n",
    "\r\n",
    "  Xtr, Ytr, Xvalid, Yvalid, Xtest, Ytest = get_dataset(step, feature, y_city=city, valid_split=1024)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yl9e5tQalzjp"
   },
   "source": [
    "## Tensorized Transformer Experiments\r\n",
    "\r\n",
    "** Run this one on TPU **"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aUxsxY5wlBzO"
   },
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as kr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "import model.tensorized_transformer as tt\n",
    "import dataset_tools.split\n",
    "from visualization_tools.visualization import visualize_pos_encoding, attention_plotter\n",
    "\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "strategy = tf.distribute.TPUStrategy(tpu)#tf.distribute.experimental.TPUStrategy(tpu)\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qH0nE0YktgWn"
   },
   "source": [
    "###### ALL PARAMETERS HERE######:\n",
    "softmax_type = 3\n",
    "epoch = 2\n",
    "patience = 20\n",
    "\n",
    "warmup_steps = 50\n",
    "factor1=-0.6\n",
    "factor2=-1.5\n",
    "\n",
    "head_num = 32\n",
    "d_model = 512\n",
    "dense_units = 512\n",
    "batch_size = 16\n",
    "\n",
    "initializer = 'RandomNormal'\n",
    "\n",
    "input_length = Xtr.shape[1]\n",
    "output_size = Ytr.shape[-1]\n",
    "\n",
    "print(f'Xtr: {Xtr.shape}')\n",
    "print(f'Ytr: {Ytr.shape}')\n",
    "print(f'Xvalid: {Xvalid.shape}')\n",
    "print(f'Yvalid: {Yvalid.shape}')\n",
    "print(f'Xtest: {Xtest.shape}')\n",
    "print(f'Ytest: {Ytest.shape}')\n",
    "\n",
    "num_examples = (Xtr.shape[0] // (batch_size * 8)) * (batch_size * 8)\n",
    "num_valid_examples = (Xvalid.shape[0] // (batch_size * 8)) * (batch_size * 8)\n",
    "\n",
    "Xtr = Xtr[:num_examples, ...]\n",
    "Ytr = Ytr[:num_examples, ...]\n",
    "Xvalid = Xvalid[:num_valid_examples, ...]\n",
    "Yvalid = Yvalid[:num_valid_examples, ...]\n",
    "\n",
    "input_shape = (input_length, Xtr.shape[-2], Xtr.shape[-1])\n",
    "output_shape = (1, 1)\n",
    "\n",
    "learning_rate = tt.CustomSchedule(d_model, warmup_steps=warmup_steps, factor1=factor1, factor2=factor2)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, \n",
    "                                     beta_1=0.9, \n",
    "                                     beta_2=0.98, \n",
    "                                     epsilon=1e-9\n",
    "                                     )\n",
    "lr_metric = tt.get_lr_metric(optimizer)\n",
    "\n",
    "with strategy.scope():\n",
    "  model = kr.Sequential([\n",
    "              kr.Input(shape=input_shape),\n",
    "              tt.PositionalEncoding(),\n",
    "              tt.EncoderLayer(input_length, d_model, head_num, dense_units, initializer, softmax_type, batch_size),\n",
    "              tt.EncoderLayer(input_length, d_model, head_num, dense_units, initializer, softmax_type, batch_size),\n",
    "              tt.EncoderLayer(input_length, d_model, head_num, dense_units, initializer, softmax_type, batch_size),\n",
    "              kr.layers.Flatten(),\n",
    "              kr.layers.Dense(tf.reduce_prod(output_shape), activation='linear'),\n",
    "              kr.layers.Reshape(output_shape),\n",
    "              ])\n",
    "  model.compile(optimizer=optimizer, loss='mae', metrics=['mse', lr_metric])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "print_attention_weights = kr.callbacks.LambdaCallback(\n",
    "    on_train_end=lambda batch: print(model.layers[1].attention_weights))\n",
    "early_stopping = kr.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                            patience=patience,\n",
    "                                            restore_best_weights=True,\n",
    "                                            verbose = 1)\n",
    "\n",
    "model.fit(\n",
    "    Xtr, Ytr,\n",
    "    epochs=epoch,\n",
    "    batch_size=batch_size * 8,\n",
    "    validation_data=(Xvalid, Yvalid),\n",
    "    callbacks=[early_stopping]\n",
    "         )"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "p2rcdY2ewHVd"
   },
   "source": [
    "date = datetime.datetime.now().strftime(\"%Y_%m_%d\")\n",
    "time = datetime.datetime.now().strftime(\"%H_%M_%S\")\n",
    "name = \"TT_\" + time\n",
    "folder = PATH + 'Tests/' + date +\"/\"\n",
    "\n",
    "listPrintables = [\n",
    "                  ['Xvalid.shape', Xvalid.shape],\n",
    "                  ['\\nepoch', epoch],\n",
    "                  ['patience', patience],\n",
    "                  ['batch_size', batch_size],\n",
    "                  ['\\nhead_num', head_num],\n",
    "                  ['d_model', d_model],\n",
    "                  ['dense_units', dense_units],\n",
    "                  ['\\nwarmup_steps', warmup_steps],\n",
    "                  ['factor1', factor1],\n",
    "                  ['factor2', factor2],\n",
    "]\n",
    "\n",
    "save_results(listPrintables, model, folder, name)\n",
    "\n",
    "pred = model.predict(Xvalid)\n",
    "plot_predictions(Yvalid, pred, folder, name, '_1.png')\n",
    "pred = model.predict(Xtest)\n",
    "plot_predictions(Ytest, pred, folder, name, '_2.png')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7O53vu1PFna"
   },
   "source": [
    "## Vanilla Transformer Experiments\r\n",
    "\r\n",
    "** Run this one on GPU **"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Q5NGO47SPFnb"
   },
   "source": [
    "import vanilla_transformer.transformer as vt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as kr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import vanilla_transformer.transformer as vt\n",
    "import dataset_tools.split\n",
    "from visualization_tools.visualization import visualize_pos_encoding, attention_plotter\n",
    "\n",
    "import datetime"
   ],
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "otfLhrDBPFnc"
   },
   "source": [
    "###### ALL PARAMETERS HERE######:\n",
    "epoch = 1 # 20\n",
    "patience = 20\n",
    "\n",
    "head_num = 32\n",
    "d_model = 512\n",
    "dense_units = 512\n",
    "batch_size = 16 * 8\n",
    "dropout_rate = 0.01\n",
    "num_layers = 3\n",
    "initializer = 'RandomNormal'\n",
    "\n",
    "Xtr_flat, Xtest_flat, Xvalid_flat = to_flatten_dataset(Xtr, Xtest, Xvalid)\n",
    "\n",
    "input_length = Xtr_flat.shape[1]\n",
    "input_size = Xtr_flat.shape[2]\n",
    "output_size = Ytr.shape[-1]\n",
    "\n",
    "Xtr_flat, Ytr = reshape_to_batches(Xtr_flat, Ytr, batch_size)\n",
    "Xvalid_flat, Yvalid = reshape_to_batches(Xvalid_flat, Yvalid, batch_size)\n",
    "\n",
    "print(f'Xtr_flat: {Xtr_flat.shape}')\n",
    "print(f'Ytr: {Ytr.shape}')\n",
    "print(f'Xvalid_flat: {Xvalid_flat.shape}')\n",
    "print(f'Yvalid: {Yvalid.shape}')\n",
    "print(f'Xtest: {Xtest_flat.shape}')\n",
    "print(f'Ytest: {Ytest.shape}')\n",
    "\n",
    "\n",
    "learning_rate = vt.CustomSchedule(d_model)\n",
    "optimizer = kr.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "model = vt.Transformer(input_size, num_layers, d_model, head_num, dense_units, input_length, output_size,\n",
    "                              rate=dropout_rate)\n",
    "model.compile()\n",
    "\n",
    "early_stopping = kr.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                            patience=patience,\n",
    "                                            restore_best_weights=True,\n",
    "                                            verbose = 1)\n",
    "\n",
    "model.fit(Xtr_flat, Ytr, \n",
    "          validation_data=(Xvalid_flat, Yvalid),\n",
    "          epochs=epoch, \n",
    "          optimizer=optimizer, \n",
    "          loss=kr.losses.MeanSquaredError(), \n",
    "          metrics={'mse': kr.metrics.mse, 'mae': kr.metrics.mae}, \n",
    "          callbacks=[early_stopping])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o2l3ECMmPFnd"
   },
   "source": [
    "date = datetime.datetime.now().strftime(\"%Y_%m_%d\")\n",
    "time = datetime.datetime.now().strftime(\"%H_%M_%S\")\n",
    "name = \"VT_\" + time\n",
    "folder = PATH + 'Tests/' + date +\"/\"\n",
    "\n",
    "listPrintables = [\n",
    "                  ['y_feature', y_feature],\n",
    "                  ['y_city', y_city],\n",
    "                  ['end_city', end_city],\n",
    "                  ['input_length', input_length],\n",
    "                  ['epoch', epoch],\n",
    "                  ['\\nhead_num', head_num],\n",
    "                  ['d_model', d_model],\n",
    "                  ['dense_units', dense_units],\n",
    "                  ['batch_size', batch_size],\n",
    "                  ['\\nnum_examples', Xtr.shape[0]],\n",
    "                  ['num_valid_examples', Xvalid.shape[0]],\n",
    "                  ['input_shape', Xtr.shape[1:]],\n",
    "                  ['patience', patience],\n",
    "]\n",
    "\n",
    "save_results(listPrintables, model, folder, name)\n",
    "pred = model.predict(Xtest_flat)[0]\n",
    "plot_predictions(Ytest, pred, folder, name, '_1.png')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FR-UFRAxEGPs"
   },
   "source": [
    "## MultiConv Experiments\r\n",
    "\r\n",
    "** Run this one on GPU **"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Yx0pzemHEFw-"
   },
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from utils import data_loader_wind_us\n",
    "from models import wind_models\n",
    "from tqdm import tqdm\n",
    "import scipy.io as sio\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "\n",
    "def train_wind_us(epochs, input_timesteps, step, test_size, city,\n",
    "                  feature, dev=torch.device(\"cpu\"), earlystopping=None, learning_rate=0.001,\n",
    "                  kernels_per_layer=16, hidden_neurons=128, batch_size=64):\n",
    "\n",
    "    print(f\"Device: {dev}\")\n",
    "\n",
    "\n",
    "    Xtr, Ytr, Xvalid, Yvalid, Xtest, Ytest = get_dataset(step, feature, y_city=city, valid_split=100)\n",
    "\n",
    "    num_output_channel = 1\n",
    "    num_features = Xtr.shape[-1]\n",
    "    num_cities = Xtr.shape[-2]\n",
    "    print(\"nf\")\n",
    "    print(num_features)\n",
    "\n",
    "    Xtr, Ytr = reshape_to_batches(Xtr, Ytr, batch_size)\n",
    "    Xvalid, Yvalid = reshape_to_batches(Xvalid, Yvalid, batch_size)\n",
    "\n",
    "    print(f'Xtr: {Xtr.shape}')\n",
    "    print(f'Ytr: {Ytr.shape}')\n",
    "    print(f'Xvalid: {Xvalid.shape}')\n",
    "    print(f'Yvalid: {Yvalid.shape}')\n",
    "\n",
    "    Xtr = torch.as_tensor(Xtr).float()\n",
    "    Ytr = torch.as_tensor(Ytr).float()\n",
    "    Xvalid = torch.as_tensor(Xvalid).float()\n",
    "    Yvalid = torch.as_tensor(Yvalid).float()\n",
    "\n",
    "\n",
    "\n",
    "    ### Model definition ###\n",
    "    model = wind_models.MultidimConvNetwork(channels=input_timesteps, height=num_features, width=num_cities,\n",
    "                                            output_channels=num_output_channel, kernels_per_layer=kernels_per_layer,\n",
    "                                            hidden_neurons=hidden_neurons)\n",
    "\n",
    "    # print(\"Parameters: \", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "    summary(model, (input_timesteps, num_cities, num_features), device=\"cpu\")\n",
    "    # Put the model on GPU\n",
    "    model.to(dev)\n",
    "    # Define optimizer\n",
    "    opt = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # Loss function\n",
    "    # loss_func = F.mse_loss\n",
    "    loss_func = F.l1_loss\n",
    "    #### Training ####\n",
    "    best_val_loss = 1e300\n",
    "\n",
    "    earlystopping_counter = 0\n",
    "    # pbar = tqdm(range(epochs), desc=\"Epochs\")\n",
    "    for epoch in range(epochs):\n",
    "        print(f'epoch: {epoch + 1}/{epochs}')\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        total_num = 0\n",
    "\n",
    "        for i, (xb, yb) in enumerate(list(zip(Xtr, Ytr))):\n",
    "            loss, num = loss_batch(model, loss_func, xb.to(dev), yb.to(dev), opt)\n",
    "            if loss_func == F.l1_loss:\n",
    "                num = 1\n",
    "            train_loss += loss\n",
    "            total_num += num\n",
    "        train_loss /= total_num\n",
    "\n",
    "        # Calc validation loss\n",
    "        val_loss = 0.0\n",
    "        val_num = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in list(zip(Xvalid, Yvalid)):\n",
    "\n",
    "                loss, num = loss_batch(model, loss_func, xb.to(dev), yb.to(dev))\n",
    "                if loss_func == F.l1_loss:\n",
    "                    num = 1\n",
    "                val_loss += loss\n",
    "                val_num += num\n",
    "            val_loss /= val_num\n",
    "\n",
    "        # pbar.set_postfix({'train_loss': train_loss, 'val_loss': val_loss})\n",
    "        print(f'train_loss: {train_loss}, val_loss: {val_loss}')\n",
    "\n",
    "        # Save the model with the best validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            earlystopping_counter = 0\n",
    "\n",
    "        else:\n",
    "            if earlystopping is not None:\n",
    "                earlystopping_counter += 1\n",
    "                if earlystopping_counter >= earlystopping:\n",
    "                    print(f\"Stopping early --> val_loss has not decreased over {earlystopping} epochs\")\n",
    "                    break\n",
    "    return model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CNRE8OgHEQmu"
   },
   "source": [
    "#Dataset specific\n",
    "input_timesteps = 4\n",
    "\n",
    "feature = 'wind'  # 'temp' or 'wind'\n",
    "step = 2  # 1=6h, 2=12h, 3=18h, 4=24h\n",
    "city = 0  # None=all or 0, 1, 2\n",
    "\n",
    "learning_rate = 0.001\n",
    "kernels_per_layer = 128\n",
    "hidden_neurons = 512\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "epochs = 2\n",
    "early_stopping = 20\n",
    "\n",
    "test_size = 8000\n",
    "display_results_after = 200\n",
    "\n",
    "\n",
    "\n",
    "train_model = True\n",
    "load_model_path = \"/content/drive/My Drive/Colab Notebooks/Tensorized Transformers/Model/sc/model_MultidimConvNetwork.pt\"\n",
    "\n",
    "\n",
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "\n",
    "if train_model:\n",
    "    model = train_wind_us(test_size=test_size, city=city,\n",
    "                  feature=feature, epochs=epochs, input_timesteps=input_timesteps,\n",
    "                  step=step, dev=dev, earlystopping=early_stopping, batch_size = batch_size,\n",
    "                  learning_rate=learning_rate, kernels_per_layer=kernels_per_layer, hidden_neurons=hidden_neurons)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jsNY8CtxLFkl"
   },
   "source": [
    "Xtr, Ytr, Xvalid, Yvalid, Xtest, Ytest = get_dataset(step, feature, y_city=city, valid_split=100)\n",
    "\n",
    "num_output_channel = 1\n",
    "num_features = Xtr.shape[-1]\n",
    "num_cities = Xtr.shape[-2]\n",
    "print(\"nf\")\n",
    "print(num_features)\n",
    "\n",
    "\n",
    "Xtest, Ytest = reshape_to_batches(Xtest, Ytest, batch_size)\n",
    "\n",
    "print(f'Xtest: {Xtest.shape}')\n",
    "print(f'Ytest: {Ytest.shape}')\n",
    "\n",
    "Xtest = torch.as_tensor(Xtest).float()\n",
    "Ytest = torch.as_tensor(Ytest).float()\n",
    "\n",
    "\n",
    "print(\"\\n\\n######################## Model description ################################\")\n",
    "summary(model, (input_timesteps, num_cities, num_features), device=\"cuda\")\n",
    "print(\"Feature = \", feature)\n",
    "print(\"Input_length = \", input_timesteps)\n",
    "print(\"Epoch = \", epochs)\n",
    "\n",
    "\n",
    "print(\"LR = \", learning_rate)\n",
    "print(\"batch_size = \", batch_size)\n",
    "\n",
    "print(\"num_examples_test = \", Xtest.shape[0])\n",
    "print(\"input_shape = \", Xtest.shape[1:])\n",
    "\n",
    "print(\"\\n\\n######################## Results ##########################################\")\n",
    "\n",
    "pred = model(Xtest[0].to('cuda'))\n",
    "plot_predictions(Ytest[0], pred)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}