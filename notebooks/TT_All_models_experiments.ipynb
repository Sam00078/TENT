{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TT_All_models_experiments.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "WF87tWVHk3l6",
        "QCbok4hOM1tP",
        "RGHq5grV2epw",
        "HTCskCqW_jnE"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6NYTcAvln4J"
      },
      "source": [
        "### Mount Google Drive\n",
        "\n",
        "**Requires dataset_tensor.npy file in \"Colab Notebooks/Tensorized Transformers/Data\" folder!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XNfw-0EfZ4B"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import scipy.io\r\n",
        "import sklearn.model_selection\r\n",
        "import datetime\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow.keras as kr\r\n",
        "\r\n",
        "! pip install -q pyyaml h5py  # Required to save models in HDF5 format\r\n",
        "! pip install torch\r\n",
        "! pip install einops\r\n",
        "! pip install tqdm\r\n",
        "! pip install torchsummary\r\n",
        "! pip install scipy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaNZqkhqfjiJ"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "PATH = '/content/drive/My Drive/Colab Notebooks/Tensorized Transformers/'\n",
        "DATA_PATH = PATH + 'Data/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHxUZWqils8I"
      },
      "source": [
        "### Clone Tensorized Transformers and multidim conv github repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmKol2g8WU1d"
      },
      "source": [
        "git_username = ''\n",
        "git_token =  ''\n",
        "\n",
        "if git_username == '':\n",
        "  print('Github username:')\n",
        "  git_username = %sx read -p ''\n",
        "  git_username = git_username[0]\n",
        "\n",
        "if git_token == '':\n",
        "  print('Github access token (https://github.com/settings/tokens):')\n",
        "  print('Github Token:')\n",
        "  git_token = %sx read -p ''\n",
        "  git_token = git_token[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KA6Ej0xOjzbu"
      },
      "source": [
        "# Clone the entire repo.\n",
        "%cd /content\n",
        "!git clone -l -s https://$git_username:$git_token@github.com/onurbil/tensorized_transformers.git tensorized_transformers\n",
        "%cd tensorized_transformers\n",
        "!ls\n",
        "%cd ..\n",
        "\n",
        "%cd /content\n",
        "!git clone -l -s https://github.com/onurbil/multidim_conv.git sc\n",
        "%cd sc\n",
        "!ls\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmfy2j6FkjLo"
      },
      "source": [
        "import sys\n",
        "\n",
        "TT_REPO_PATH = '/content/tensorized_transformers'\n",
        "SC_REPO_PATH = '/content/sc'\n",
        "\n",
        "sys.path.append(TT_REPO_PATH)\n",
        "sys.path.append(SC_REPO_PATH)\n",
        "print(sys.path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF87tWVHk3l6"
      },
      "source": [
        "### Get Kaggle data and save it to your Drive\r\n",
        "\r\n",
        "** Only if you don't have it saved in your drive or want to update it **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuLsnW-uRIyf"
      },
      "source": [
        "import shutil\r\n",
        "import os\r\n",
        "from common.paths import PROCESSED_DATASET_DIR\r\n",
        "from common.paths import EU_PROCESSED_DATASET_DIR\r\n",
        "from tensorized_transformers import main\r\n",
        "\r\n",
        "filesToMoveUS = ['dataset_tensor.npy',\r\n",
        "               'scale.npy']\r\n",
        "filesToMoveEU = ['eu_dataset_tensor.npy',\r\n",
        "               'eu_scale.npy']\r\n",
        "\r\n",
        "os.makedirs(os.path.dirname(DATA_PATH), exist_ok=True)\r\n",
        "for files in filesToMoveUS:\r\n",
        "  shutil.copy(PROCESSED_DATASET_DIR + '/' + files, DATA_PATH)\r\n",
        "for files in filesToMoveEU:\r\n",
        "  shutil.copy(EU_PROCESSED_DATASET_DIR + '/' + files, DATA_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCbok4hOM1tP"
      },
      "source": [
        "## Tensorized Transformer\r\n",
        "** Run on TPU **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3AN3GGHExYm"
      },
      "source": [
        "import experiment_tools.load_dataset as load_dataset\r\n",
        "import experiment_tools.tt_training as tt_training\r\n",
        "\r\n",
        "# dataset\r\n",
        "input_length = 16\r\n",
        "prediction_time = 4\r\n",
        "y_feature = 4\r\n",
        "y_city = 1 \r\n",
        "num_cities = 30\r\n",
        "remove_last_from_test= 800 \r\n",
        "valid_size = 1024\r\n",
        "\r\n",
        "# model\r\n",
        "softmax_type = 3\r\n",
        "epoch = 1 # 300\r\n",
        "patience = 20\r\n",
        "num_layers = 3\r\n",
        "head_num = 32\r\n",
        "d_model = 256\r\n",
        "dense_units = 128\r\n",
        "batch_size = 16\r\n",
        "loss = 'mse'\r\n",
        "\r\n",
        "dataset, dataset_params = load_dataset.get_usa_dataset(DATA_PATH, input_length, prediction_time, \r\n",
        "                                       y_feature, y_city, \r\n",
        "                                       end_city=num_cities, \r\n",
        "                                       remove_last_from_test=remove_last_from_test, \r\n",
        "                                       valid_split=valid_size, split_random=1337)\r\n",
        "\r\n",
        "model, model_params, history = tt_training.train_model(dataset, \r\n",
        "                                                       softmax_type, epoch, patience, \r\n",
        "                                                       num_layers, head_num, d_model, dense_units, \r\n",
        "                                                       batch_size, loss, use_tpu=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcJCcTSyK0Rn"
      },
      "source": [
        "import experiment_tools.results as results\r\n",
        "\r\n",
        "params = dataset_params + model_params\r\n",
        "results.print_params(params)\r\n",
        "\r\n",
        "folder = results.save_results_with_datetime(model, 'TT', PATH, params)\r\n",
        "\r\n",
        "Xtr, Ytr, Xvalid, Yvalid, Xtest, Ytest = dataset\r\n",
        "results.plot_valid_test_predictions(model, Xvalid, Yvalid, Xtest, Ytest, y_feature, folder, 'TT')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGHq5grV2epw"
      },
      "source": [
        "## Vanilla Transformer\r\n",
        "** Run on GPU **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLW9Jgh62epy"
      },
      "source": [
        "import experiment_tools.load_dataset as load_dataset\r\n",
        "import experiment_tools.vanilla_training as vanilla_training\r\n",
        "\r\n",
        "# dataset\r\n",
        "input_length = 16\r\n",
        "prediction_time = 4\r\n",
        "y_feature = 4\r\n",
        "y_city = 1 \r\n",
        "num_cities = 30\r\n",
        "remove_last_from_test= 800 \r\n",
        "valid_size = 1024\r\n",
        "\r\n",
        "# model\r\n",
        "epoch = 1 # 300\r\n",
        "patience = 20\r\n",
        "num_layers = 3 \r\n",
        "head_num = 32\r\n",
        "d_model = 512\r\n",
        "dense_units = 512\r\n",
        "dropout_rate = 0.01\r\n",
        "batch_size = 128\r\n",
        "loss = kr.losses.mean_squared_error\r\n",
        "\r\n",
        "\r\n",
        "dataset, dataset_params = load_dataset.get_usa_dataset(DATA_PATH, input_length, prediction_time, \r\n",
        "                                       y_feature, y_city, \r\n",
        "                                       end_city=num_cities, \r\n",
        "                                       remove_last_from_test=remove_last_from_test, \r\n",
        "                                       valid_split=valid_size, split_random=1337)\r\n",
        "\r\n",
        "model, model_params = vanilla_training.train_model(dataset, \r\n",
        "                                                   epoch, patience,\r\n",
        "                                                   num_layers, head_num,\r\n",
        "                                                   d_model, dense_units,\r\n",
        "                                                   batch_size, dropout_rate,\r\n",
        "                                                   loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Es3bMmBw2epz"
      },
      "source": [
        "import experiment_tools.results as results\r\n",
        "import experiment_tools.load_dataset as experiment_dataset\r\n",
        "\r\n",
        "params = dataset_params + model_params\r\n",
        "results.print_params(params)\r\n",
        "\r\n",
        "folder = results.save_results_with_datetime(model, 'Vanilla', PATH, params)\r\n",
        "\r\n",
        "Xtr, Ytr, Xvalid, Yvalid, Xtest, Ytest = dataset\r\n",
        "Xtr_flat, Xtest_flat, Xvalid_flat = experiment_dataset.to_flatten_dataset(Xtr, Xtest, Xvalid)\r\n",
        "\r\n",
        "results.plot_valid_test_predictions(model, Xvalid_flat, Yvalid, Xtest_flat, Ytest, y_feature, folder, 'Vanilla', model_returns_activations=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTCskCqW_jnE"
      },
      "source": [
        "## 3D CNN\r\n",
        "** Run on GPU **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtY7B5NI_kMc"
      },
      "source": [
        "import experiment_tools.load_dataset as load_dataset\r\n",
        "import experiment_tools.cnn3d_training as cnn3d_training\r\n",
        "\r\n",
        "# dataset\r\n",
        "input_length = 16\r\n",
        "prediction_time = 4\r\n",
        "y_feature = 4\r\n",
        "y_city = 1 \r\n",
        "num_cities = 30\r\n",
        "remove_last_from_test= 800 \r\n",
        "valid_size = 1024\r\n",
        "\r\n",
        "# model\r\n",
        "epoch = 1 # 300\r\n",
        "patience = 20\r\n",
        "filters = 10\r\n",
        "kernel_size = 2\r\n",
        "batch_size = 128\r\n",
        "learning_rate = 0.0001\r\n",
        "loss='mse'\r\n",
        "\r\n",
        "dataset, dataset_params = load_dataset.get_usa_dataset(DATA_PATH, input_length, prediction_time, \r\n",
        "                                       y_feature, y_city, \r\n",
        "                                       end_city=num_cities, \r\n",
        "                                       remove_last_from_test=remove_last_from_test, \r\n",
        "                                       valid_split=valid_size, split_random=1337)\r\n",
        "\r\n",
        "model, model_params, history = cnn3d_training.train_model(dataset, \r\n",
        "                                                          epoch, patience,\r\n",
        "                                                          filters, kernel_size,\r\n",
        "                                                          batch_size, \r\n",
        "                                                          learning_rate, loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptBxt1WbARQb"
      },
      "source": [
        "import experiment_tools.results as results\r\n",
        "import experiment_tools.cnn3d_training as cnn3d_training\r\n",
        "\r\n",
        "params = dataset_params + model_params\r\n",
        "results.print_params(params)\r\n",
        "\r\n",
        "folder = results.save_results_with_datetime(model, 'CNN3D', PATH, params)\r\n",
        "\r\n",
        "Xtr, Ytr, Xvalid, Yvalid, Xtest, Ytest = dataset\r\n",
        "Xtr_t, Xvalid_t, Xtest_t = cnn3d_training.transform_dataset(Xtr, Xvalid, Xtest)\r\n",
        "\r\n",
        "results.plot_valid_test_predictions(model, Xvalid_t, Yvalid, Xtest_t, Ytest, y_feature, folder, 'CNN3D')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR-UFRAxEGPs"
      },
      "source": [
        "## MultiConv Experiments\r\n",
        "\r\n",
        "** Run on GPU **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkBupG-LnbX1"
      },
      "source": [
        "import dataset_tools.split\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "def get_usa_dataset(lag, step, y_feature, y_city, start_city=0, end_city=30, remove_last_from_test=0, valid_split=None, split_random=None):\n",
        "  filename = DATA_PATH + 'dataset_tensor.npy'\n",
        "  dataset = np.load(filename, allow_pickle=True)\n",
        "\n",
        "  print(dataset.shape)\n",
        "\n",
        "  train, test = dataset_tools.split.split_train_test(dataset)\n",
        "  x_train, y_train = dataset_tools.split.get_xy(train, input_length=lag, pred_time=step)\n",
        "  x_test, y_test = dataset_tools.split.get_xy(test, input_length=lag, pred_time=step)\n",
        "\n",
        "  print(x_train.shape, y_train.shape)\n",
        "  if valid_split is not None:\n",
        "    x_train, x_valid, y_train, y_valid = sklearn.model_selection.train_test_split(x_train, y_train, test_size=valid_split, random_state=split_random)\n",
        "    x_valid = np.reshape(x_valid, (x_valid.shape[0], x_valid.shape[1], dataset.shape[1], dataset.shape[2]))\n",
        "    y_valid = np.reshape(y_valid, (y_valid.shape[0], dataset.shape[1], dataset.shape[2]))\n",
        "    x_valid = x_valid[:, :, start_city:end_city, :]\n",
        "    y_valid = y_valid[:, start_city:end_city, :]\n",
        "    y_valid = np.expand_dims(y_valid[..., y_city, y_feature], axis=-1)\n",
        "\n",
        "  x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], dataset.shape[1], dataset.shape[2]))\n",
        "  y_train = np.reshape(y_train, (y_train.shape[0], dataset.shape[1], dataset.shape[2]))\n",
        "  x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], dataset.shape[1], dataset.shape[2]))\n",
        "  y_test = np.reshape(y_test, (y_test.shape[0], dataset.shape[1], dataset.shape[2]))\n",
        "\n",
        "  x_train = x_train[:, :, start_city:end_city, :]\n",
        "  y_train = y_train[:, start_city:end_city, :]\n",
        "  x_test = x_test[:-remove_last_from_test, :, start_city:end_city, :]\n",
        "  y_test = y_test[:-remove_last_from_test, start_city:end_city, :]\n",
        "\n",
        "  print(f'FULL_x_train.shape: {x_train.shape}')\n",
        "\n",
        "  y_train = np.expand_dims(y_train[..., y_city, y_feature], axis=-1)\n",
        "  y_test = np.expand_dims(y_test[..., y_city, y_feature], axis=-1)\n",
        "\n",
        "  if valid_split is None:\n",
        "    return x_train, y_train, x_test, y_test\n",
        "  else:\n",
        "    return x_train, y_train, x_valid, y_valid, x_test, y_test\n",
        "\n",
        "def get_denmark_dataset(step, feature, y_city=None, valid_split=None, split_random=None):\n",
        "  filename = DATA_PATH + f'Denmark/{feature}/step{step}.mat'\n",
        "  mat = scipy.io.loadmat(filename)\n",
        "  Xtr = mat['Xtr'].swapaxes(1, 2)\n",
        "  Ytr = mat['Ytr']\n",
        "  Xtest = mat['Xtest'].swapaxes(1, 2)\n",
        "  Ytest = mat['Ytest']\n",
        "  if y_city is not None:\n",
        "    Ytr = Ytr[:, y_city:y_city + 1]\n",
        "    Ytest = Ytest[:, y_city:y_city + 1]\n",
        "\n",
        "  if valid_split is None:\n",
        "    return Xtr, Ytr, Xtest, Ytest\n",
        "  else:\n",
        "    Xtr, Xvalid, Ytr, Yvalid = sklearn.model_selection.train_test_split(Xtr, Ytr, test_size=valid_split, random_state=split_random)\n",
        "    return Xtr, Ytr, Xvalid, Yvalid, Xtest, Ytest\n",
        "\n",
        "\n",
        "def to_flatten_dataset(Xtr, Xtest, Xvalid=None):\n",
        "  t = Xtr.shape[1]\n",
        "  f = Xtr.shape[2] * Xtr.shape[3]\n",
        "\n",
        "  Xtr = Xtr.reshape((Xtr.shape[0], t, f))\n",
        "  Xtest = Xtest.reshape((Xtest.shape[0], t, f))\n",
        "  if Xvalid is None:\n",
        "    return Xtr, Xtest\n",
        "  else:\n",
        "    print(Xvalid.shape)\n",
        "    Xvalid = Xvalid.reshape((Xvalid.shape[0], t, f))\n",
        "    return Xtr, Xtest, Xvalid\n",
        "\n",
        "\n",
        "def reshape_to_batches(Xs, Ys, batch_size):\n",
        "  num_batches = Xs.shape[0] // batch_size\n",
        "  batched_len = num_batches * batch_size\n",
        "  print(Ys.shape)\n",
        "\n",
        "  Xs = Xs[:batched_len, ...].reshape((num_batches, batch_size) + Xs.shape[1:])\n",
        "  Ys = Ys[:batched_len, ...].reshape((num_batches, batch_size) + Ys.shape[1:])\n",
        "\n",
        "  return Xs, Ys\n",
        "\n",
        "def plot_predictions(Ys, pred, folder, name, ending):\n",
        "  fileName = folder + name \n",
        "  pred = pred.flatten()\n",
        "  Ys = Ys.flatten()\n",
        "  mae = kr.metrics.mae(Ys, pred)\n",
        "  mse = kr.metrics.mse(Ys, pred)\n",
        "  f = open(fileName + \".txt\", \"a\")\n",
        "  print(f'Figure mae{ending}: {np.mean(mae)}', file=f)\n",
        "  print(f'Figure mse{ending}: {np.mean(mse)}', file=f)\n",
        "  print('\\n\\n', file=f)\n",
        "  f.close()\n",
        "  print(f'Figure mae: {np.mean(mae)}')\n",
        "  print(f'Figure mse: {np.mean(mse)}')\n",
        "\n",
        "  plot_width = 20 if Ys.size < 1000 else 100\n",
        "  plt.figure(figsize=(plot_width, 8))\n",
        "  plt.plot(range(pred.size), pred, label='pred')\n",
        "  plt.plot(range(len(Ys)), Ys, label='true')\n",
        "  plt.legend()\n",
        "  plt.savefig(fileName + ending)\n",
        "  plt.show()\n",
        "  \n",
        "def save_results(listPrintables, model, folder, name):\n",
        "  fileName = folder + name \n",
        "\n",
        "  os.makedirs(os.path.dirname(folder), exist_ok=True)\n",
        "  with open(fileName + \".txt\", \"a\") as fh:\n",
        "    model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
        "  f = open(fileName + \".txt\", \"a\")\n",
        "  print(\"\\n\\n######################## Model description ################################\", file=f)\n",
        "  for name, results in listPrintables:\n",
        "    print(str(name) + \" = \" + str(results), file=f)\n",
        "  print(\"######################### Results ##########################################\", file=f)\n",
        "  f.close()\n",
        "\n",
        "\n",
        "Xtr, Ytr, Xvalid, Yvalid, Xtest, Ytest = get_usa_dataset(lag=16, step=4, \n",
        "                                                         y_feature=4, y_city=0, \n",
        "                                                         start_city=0, end_city=30, \n",
        "                                                         remove_last_from_test=800, \n",
        "                                                         valid_split=1024, split_random=None)\n",
        "# Xtr, Xtest, Xvalid = to_flatten_dataset(Xtr, Xtest, Xvalid)\n",
        "# Xtr, Ytr = reshape_to_batches(Xtr, Ytr, 128)\n",
        "# print(Xtr.shape, Ytr.shape, Xtest.shape, Ytest.shape, Xvalid.shape, Yvalid.shape)\n",
        "\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.figure(figsize=(100, 8))\n",
        "# plt.plot(range(Ytest.size), Ytest.flatten())\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yx0pzemHEFw-"
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from utils import data_loader_wind_us\n",
        "from models import wind_models\n",
        "from tqdm import tqdm\n",
        "import scipy.io as sio\n",
        "from torchsummary import summary\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
        "    loss = loss_func(model(xb), yb)\n",
        "\n",
        "    if opt is not None:\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "    return loss.item(), len(xb)\n",
        "\n",
        "\n",
        "def train_wind_us(dataset, epochs, dev=torch.device(\"cpu\"), patience=None, \n",
        "                  learning_rate=0.001,\n",
        "                  kernels_per_layer=16, hidden_neurons=128, batch_size=64):\n",
        "\n",
        "    print(f\"Device: {dev}\")\n",
        "\n",
        "\n",
        "    Xtr, Ytr, Xvalid, Yvalid, Xtest, Ytest = dataset\n",
        "\n",
        "    input_length = Xtr.shape[1]\n",
        "    num_output_channel = 1\n",
        "    num_features = Xtr.shape[-1]\n",
        "    num_cities = Xtr.shape[-2]\n",
        "    print(\"nf\")\n",
        "    print(num_features)\n",
        "\n",
        "    Xtr, Ytr = reshape_to_batches(Xtr, Ytr, batch_size)\n",
        "    Xvalid, Yvalid = reshape_to_batches(Xvalid, Yvalid, batch_size)\n",
        "\n",
        "    print(f'Xtr: {Xtr.shape}')\n",
        "    print(f'Ytr: {Ytr.shape}')\n",
        "    print(f'Xvalid: {Xvalid.shape}')\n",
        "    print(f'Yvalid: {Yvalid.shape}')\n",
        "\n",
        "    Xtr = torch.as_tensor(Xtr).float()\n",
        "    Ytr = torch.as_tensor(Ytr).float()\n",
        "    Xvalid = torch.as_tensor(Xvalid).float()\n",
        "    Yvalid = torch.as_tensor(Yvalid).float()\n",
        "\n",
        "\n",
        "\n",
        "    ### Model definition ###\n",
        "    model = wind_models.MultidimConvNetwork(channels=input_length, height=num_features, width=num_cities,\n",
        "                                            output_channels=num_output_channel, kernels_per_layer=kernels_per_layer,\n",
        "                                            hidden_neurons=hidden_neurons)\n",
        "\n",
        "    # print(\"Parameters: \", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "    summary(model, (input_length, num_cities, num_features), device=\"cpu\")\n",
        "    # Put the model on GPU\n",
        "    model.to(dev)\n",
        "    # Define optimizer\n",
        "    opt = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    # Loss function\n",
        "    # loss_func = F.mse_loss\n",
        "    loss_func = F.l1_loss\n",
        "    #### Training ####\n",
        "    best_val_loss = 1e300\n",
        "\n",
        "    early_stopping_epcch = None\n",
        "    earlystopping_counter = 0\n",
        "    # pbar = tqdm(range(epochs), desc=\"Epochs\")\n",
        "    for epoch in range(epochs):\n",
        "        print(f'epoch: {epoch + 1}/{epochs}')\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        total_num = 0\n",
        "\n",
        "        for i, (xb, yb) in enumerate(list(zip(Xtr, Ytr))):\n",
        "            loss, num = loss_batch(model, loss_func, xb.to(dev), yb.to(dev), opt)\n",
        "            if loss_func == F.l1_loss:\n",
        "                num = 1\n",
        "            train_loss += loss\n",
        "            total_num += num\n",
        "        train_loss /= total_num\n",
        "\n",
        "        # Calc validation loss\n",
        "        val_loss = 0.0\n",
        "        val_num = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in list(zip(Xvalid, Yvalid)):\n",
        "\n",
        "                loss, num = loss_batch(model, loss_func, xb.to(dev), yb.to(dev))\n",
        "                if loss_func == F.l1_loss:\n",
        "                    num = 1\n",
        "                val_loss += loss\n",
        "                val_num += num\n",
        "            val_loss /= val_num\n",
        "\n",
        "        # pbar.set_postfix({'train_loss': train_loss, 'val_loss': val_loss})\n",
        "        print(f'train_loss: {train_loss}, val_loss: {val_loss}')\n",
        "\n",
        "        # Save the model with the best validation loss\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            earlystopping_counter = 0\n",
        "\n",
        "        else:\n",
        "            if patience is not None:\n",
        "                earlystopping_counter += 1\n",
        "                if earlystopping_counter >= patience:\n",
        "                    early_stopping_epcch = epoch\n",
        "                    print(f\"Stopping early --> val_loss has not decreased over {patience} epochs\")\n",
        "                    break\n",
        "\n",
        "    params = [\n",
        "              ('epochs', epochs),\n",
        "              ('patience', patience),\n",
        "              ('early_stopping_epcch', early_stopping_epcch),\n",
        "              ('learning_rate', learning_rate),\n",
        "              ('kernels_per_layer', kernels_per_layer), \n",
        "              ('hidden_neurons', hidden_neurons), \n",
        "              ('batch_size', batch_size),\n",
        "    ]\n",
        "    return model, params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNRE8OgHEQmu"
      },
      "source": [
        "import experiment_tools.load_dataset as load_dataset\n",
        "\n",
        "# dataset\n",
        "input_length = 16\n",
        "prediction_time = 4\n",
        "y_feature = 4\n",
        "y_city = 1 \n",
        "num_cities = 30\n",
        "remove_last_from_test= 800 \n",
        "valid_size = 1024\n",
        "\n",
        "# model\n",
        "epoch = 1 # 300\n",
        "patience = 20\n",
        "learning_rate = 0.001\n",
        "kernels_per_layer = 128\n",
        "hidden_neurons = 512\n",
        "batch_size = 128\n",
        "\n",
        "dataset, dataset_params = load_dataset.get_usa_dataset(DATA_PATH, input_length, prediction_time, \n",
        "                                       y_feature, y_city, \n",
        "                                       end_city=num_cities, \n",
        "                                       remove_last_from_test=remove_last_from_test, \n",
        "                                       valid_split=valid_size, split_random=1337)\n",
        "\n",
        "load_model_path = \"/content/drive/My Drive/Colab Notebooks/Tensorized Transformers/Model/sc/model_MultidimConvNetwork.pt\"\n",
        "\n",
        "\n",
        "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "# torch.backends.cudnn.benchmark = True\n",
        "\n",
        "model, model_params = train_wind_us(dataset, epochs=epoch, dev=dev, \n",
        "                      patience=patience, batch_size = batch_size,\n",
        "                      hidden_neurons=hidden_neurons)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfP3nilCWzAW"
      },
      "source": [
        "import experiment_tools.results as results\r\n",
        "\r\n",
        "params = dataset_params + model_params\r\n",
        "results.print_params(params)\r\n",
        "\r\n",
        "Xtr, Ytr, Xvalid, Yvalid, Xtest, Ytest = dataset\r\n",
        "summary(model, Xtr.shape[-3:], device=\"cuda\")\r\n",
        "\r\n",
        "Xtest = torch.as_tensor(Xtest).float()\r\n",
        "Xvalid = torch.as_tensor(Xvalid).float()\r\n",
        "Ytest = torch.as_tensor(Ytest).float().cpu().detach().numpy()\r\n",
        "Yvalid = torch.as_tensor(Yvalid).float().cpu().detach().numpy()\r\n",
        "\r\n",
        "pred_valid = model(Xvalid.to('cuda')).cpu().detach().numpy()\r\n",
        "pred_test = model(Xtest.to('cuda')).cpu().detach().numpy()\r\n",
        "\r\n",
        "results.plot_valid_test_predictions(model, Xvalid, Yvalid, Xtest, Ytest, y_feature, PATH, 'MultiConv', pred_valid=pred_valid, pred_test=pred_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}